{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd23f502-45c6-43a5-92a2-c5b9735449ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audio covid-19  AI\n",
    "\n",
    "We modified the [Baseline] code in the following ways. \n",
    "\n",
    "1. librosa.effects.trim()함수를 이용한 전처리 - removal of silent part\n",
    "2. training데이터를 train, validation으로 split한 후, MLPClassifier로 교차 검증 후 최적의 모델 파라미터 결정 \n",
    "\n",
    "'''\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib as mpl\n",
    "import librosa.display\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') \n",
    "\n",
    "CFG = {\n",
    "    'SR':16000,\n",
    "    'N_MFCC':32, # MFCC 벡터를 추출할 개수\n",
    "    'SEED':133\n",
    "}\n",
    "\n",
    "# def seed_everything(seed):\n",
    "#     random.seed(seed)\n",
    "#     os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#     np.random.seed(seed)\n",
    "\n",
    "# seed_everything(CFG['SEED']) # Seed 고정\n",
    "\n",
    "train_df = pd.read_csv('train_data.csv')\n",
    "test_df = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aa075e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc_feature(df, data_type, save_path):\n",
    "    \n",
    "    #cm_hot = mpl.cm.get_cmap('hot')\n",
    "    \n",
    "    # Data Folder path\n",
    "    root_folder = './'\n",
    "    if os.path.exists(save_path):\n",
    "        print(f'{save_path} is exist.')\n",
    "        return\n",
    "    features = []\n",
    "    for uid in tqdm(df['id']):\n",
    "        root_path = os.path.join(root_folder, data_type)\n",
    "        path = os.path.join(root_path, str(uid).zfill(5)+'.wav')\n",
    "\n",
    "        # librosa패키지를 사용하여 wav 파일 load\n",
    "        y, sr = librosa.load(path, sr=CFG['SR'])\n",
    "        \n",
    "        # librosa패키지를 사용하여 mfcc 추출\n",
    "        # 참조 코드: https://github.com/virufy/virufy-covid/virufy_cdf_quickstart.ipynb\n",
    "        _, ints = librosa.effects.trim(y, top_db=40) \n",
    "        pad = 0.25*sr\n",
    "        start = int(max(ints[0]-pad, 0))\n",
    "        end = int(min(ints[1]+pad, len(y)))\n",
    "        y3 = y[start:end]\n",
    "\n",
    "        chunk = 4.09 # second.\n",
    "        y3_ = y3[:np.floor(chunk*sr).astype(int)]\n",
    "        y4 = np.zeros(int(sr*chunk))\n",
    "        y4[:min(len(y4), len(y3_))] = y3_[:min(len(y4), len(y3_))]        \n",
    "        mfcc4 = librosa.feature.mfcc(y=y4, sr=sr, n_mfcc=CFG['N_MFCC'])            \n",
    "        \n",
    "        print(uid, y.shape, y4.shape, mfcc4.shape)\n",
    "                \n",
    "        # 추출된 MFCC들의 평균을 Feature로 사용\n",
    "        y_feature = []\n",
    "        for e in mfcc4:\n",
    "            y_feature.append(np.mean(e))\n",
    "        features.append(y_feature)\n",
    "    \n",
    "    # 기존의 자가진단 정보를 담은 데이터프레임에 추출된 오디오 Feature를 추가\n",
    "    mfcc_df = pd.DataFrame(features, columns=['mfcc_'+str(x) for x in range(1, CFG['N_MFCC']+1)])\n",
    "    df = pd.concat([df, mfcc_df], axis=1)\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adf256a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open/train_mfcc_data3.csv is exist.\n",
      "open/test_mfcc_data3.csv is exist.\n"
     ]
    }
   ],
   "source": [
    "get_mfcc_feature(train_df, 'train', 'open/train_mfcc_data3.csv')\n",
    "get_mfcc_feature(test_df, 'test', 'open/test_mfcc_data3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f896cce2-0f47-4377-b2a1-95144e58581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wav 파일의 MFCC Feature와 상태정보를 합친 학습데이터를 불러옵니다.\n",
    "train_df = pd.read_csv('open/train_mfcc_data3.csv')\n",
    "\n",
    "# 학습데이터를 모델의 input으로 들어갈 x와 label로 사용할 y로 분할\n",
    "train_x = train_df.drop(columns=['id', 'covid19'])\n",
    "train_y = train_df['covid19']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4a66ac7-c6db-4258-802a-9fedded85412",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def onehot_encoding(ohe, x):\n",
    "    \n",
    "    # 학습데이터로 부터 fit된 one-hot encoder (ohe)를 받아 transform 시켜주는 함수\n",
    "    encoded = ohe.transform(x['gender'].values.reshape(-1,1))\n",
    "    encoded_df = pd.DataFrame(encoded, columns=ohe.categories_[0])\n",
    "    x = pd.concat([x.drop(columns=['gender']), encoded_df], axis=1)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cade4609-22a8-40f1-936d-a8ec8ba90f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# 'gender' column의 경우 추가 전처리가 필요 -> OneHotEncoder 적용\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "ohe.fit(train_x['gender'].values.reshape(-1,1))\n",
    "train_x = onehot_encoding(ohe, train_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3921a22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "elapsed...  1.4346873760223389\n",
      "{'activation': 'relu', 'alpha': 0.01, 'batch_size': 'auto', 'hidden_layer_sizes': 65, 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "test of train data: f1_macro scores = 0.6056301130071329\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "import os, sys, time\n",
    "\n",
    "## train 데이터를 대상으로 7:3으로 train/test split, random_state 값이 test결과에 영향. \n",
    "X_train, X_test, y_train, y_test=  train_test_split(train_x, train_y, test_size=0.3, random_state=1200)\n",
    "\n",
    "# print(X_train.shape, X_test.shape, train_x.shape)\n",
    "\n",
    "# 테스트한 파라미터 조합\n",
    "'''\n",
    "tuned_parameters = {\n",
    "    'activation': (['identity', 'logistic', 'tanh', 'relu']),\n",
    "    'hidden_layer_sizes': ([[40], [45], [50], [55], [60], [65], [70], [75], [80], [85], [90], [95], [100], [105], [110], [115], [120], [125], [130], [135], [140]]),\n",
    "    'alpha':     ([0.01,0.001, 0.0001]),\n",
    "    'batch_size':         ['auto'],\n",
    "    'learning_rate_init':    [0.01, 0.001],\n",
    "    'solver': (['lbfgs', 'sgd', 'adam'])\n",
    "}\n",
    "'''\n",
    "\n",
    "# 아래는 GridSearchCV 혹은 RandomizedSearchCV를 사용하여 찾은 최적의 파라미터 조합.\n",
    "tuned_parameters = {\n",
    "    'activation': ['relu'],\n",
    "    'hidden_layer_sizes': [65],\n",
    "    'alpha':     [0.01],\n",
    "    'batch_size':  ['auto'],\n",
    "    'learning_rate_init':    [0.01],\n",
    "    'solver': [\"adam\"]\n",
    "}\n",
    "\n",
    "clf =  GridSearchCV(MLPClassifier(), tuned_parameters, cv=5, n_jobs=1, scoring='f1_macro', verbose=1)  \n",
    "\n",
    "\n",
    "st = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "print('elapsed... ', time.time()-st)\n",
    "\n",
    "print(clf.best_params_)\n",
    "a = clf.best_params_\n",
    "\n",
    "clf2 = MLPClassifier(solver=a['solver'],learning_rate_init=a['learning_rate_init'], hidden_layer_sizes=a['hidden_layer_sizes'], \n",
    "                           batch_size=a['batch_size'], alpha=a['alpha'], activation=a['activation'], random_state=CFG['SEED'])\n",
    "\n",
    "clf_nn_opt = clf2.fit(X_train, y_train)\n",
    "\n",
    "## testing \n",
    "# y_pred = clf_nn_opt.predict(X_test)\n",
    "y_pred = clf2.predict(X_test)\n",
    "\n",
    "res = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'test of train data: f1_macro scores = {res}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cf251c2-6cba-446f-ab6c-45236eec93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# 위의 학습데이터를 전처리한 과정과 동일하게 test data에도 적용\n",
    "#\n",
    "\n",
    "model = clf_nn_opt # training데이터를 이용하여 최적의 파라미터로 학습한 모델\n",
    "\n",
    "test_x = pd.read_csv('open/test_mfcc_data3.csv')\n",
    "test_x = test_x.drop(columns=['id'])\n",
    "\n",
    "# Data Leakage에 유의하여 train data로만 학습된 ohe를 사용\n",
    "test_x = onehot_encoding(ohe, test_x)\n",
    "\n",
    "# Model 추론\n",
    "preds = model.predict(test_x)\n",
    "\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission['covid19'] = preds\n",
    "submission.to_csv('./submit6.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cd19e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
